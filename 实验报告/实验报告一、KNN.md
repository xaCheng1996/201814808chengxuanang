# 实验报告一、KNN

### 实验介绍：

对英文数据集进行预处理、向量空间模型（VSM）处理之后，使用KNN进行分类。

### 数据集：

20news-18828

### 字典规模：

250K

过滤词频>=5，约为4W+

过滤词频>=15, 约为两万（19367）

过滤词频>=50,为7833

本实验以词频50为基准过滤

### 实验步骤：

#### 1、预处理

TextBlob直接预处理，根据人工评测结果效果一般。

#### 2、VSM

2.1 词典处理

Bag-of-Words直接对应词典输出，每一篇文章会形成一个7833维的向量，存储中采用索引存储，示例如下：

*[207, 3947, 5256, 181, 5366, 1477, 1059, 2779, 2547, 1669, 3844, 5777, 4545, 5942, 5458, 223, 1403, 2485, 2873, 6536, 3107, 5378, 427, 994, 1324, 1849, 4521, 3394, 3394, 6536, 200, 5117, 175, 4361, 335, 4521, 5378, 6016, 1544, 309, 4348, 2547, 5779, 3587, .......]*

2.2 tf-idf

计算方法：

$$tf_w=\frac{单词w的频率}{所有词条数目}$$

$$idf=log(\frac{语料中的文档总数}{包含w的语料文档+1})$$

$$tf-idf=tf*idf$$

在具体的实现中，我们将词频直接存储在文件中，便于计算，具体如下所示：

├─.idea
│  └─inspectionProfiles
├─20news-18828
│  ├─alt.atheism
│  ├─comp.graphics
│  ├─comp.os.ms-windows.misc
├─data
├─data_statistic_every
│  ├─alt.atheism
│  ├─comp.graphics
│  ├─comp.os.ms-windows.misc
├─data_tf_idf
│  ├─alt.atheism
│  ├─comp.graphics
│  ├─comp.os.ms-windows.misc

├─Vector_space_model.py

├─data_helper.py

└─__pycache__

20news-1882等三个文件夹结构都是原始数据结构，这里只展示前三个文件夹，实际上有20类。

data_helper.py保存大部分文件读取的方法，具体说明见注释

data文件夹保存字典和词频表

data_statistic_every保存每个文件的词频，主要为了计算tf值

data_tf_idf保存每个文件的tf_idf值

#### 3、KNN

KNN使用余弦值计算相似度，从文件中读取倒排索引，由函数data_expansion将倒排索引值扩展为可计算的向量，然后直接计算相似度。

设定k=10， train/test=0.8/0.2

### 实验结果

由于将大多数文件分别存储在文件上，读取时间较长，进行一次完整的KNN计算需要数小时。

准确度acc: 0.7408885341846235（2785/3759）

